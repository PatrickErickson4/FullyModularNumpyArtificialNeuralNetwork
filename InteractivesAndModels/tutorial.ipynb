{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial for A Fully Modular Numpy-Restricted Artificial Neural Network with Adam/AdamW Optimization, Element-wise, and Input-wise Dropouts**\n",
    "Author: Patrick Erickson\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook details the Artificial Neural Network I had made from scratch, and details how to use it. We will primarily be using a breast cancer screening dataset for the majority of our model demonstrations. Note that this data is almost completely linearly seperable, and neural networks might be too complex to gain any more predictive power. This is merely for demonstrations. The model can be found on the hyperlink of the README. If wou want more information about this dataset, look at my EDA article under my logistic regression github.\n",
    "\n",
    "### For Classification\n",
    "\n",
    "**Step 1**: Load the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "scripts_path = r\"../\"\n",
    "\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "from NeuralNetworkScripts.NeuralNetwork import NeuralNetwork\n",
    "from NeuralNetworkScripts.Layer import FullyConnectedLayer\n",
    "from NeuralNetworkScripts.DataHandler import DataHelper\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: We will load in the dataset and do some preliminary cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "df = (pd.read_csv(\"../datasets/breast_cancer.csv\")).dropna()\n",
    "\n",
    "columns = df.columns.to_list()\n",
    "df.replace({'Class': {2: 0, 4: 1}}, inplace=True)\n",
    "df = df.to_numpy()\n",
    "df = df.astype(int)\n",
    "split = np.split(df, [9], axis=1)\n",
    "dataframe = split[0]\n",
    "labels = split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** We need to split, standardize, and one-hot encode our labels for model testing. We can do that with the following Data Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,_ = DataHelper.toOneHot(labels)\n",
    "\n",
    "trainSet,trainLabels,testSet,testLabels = DataHelper.trainTestSplit(dataframe,labels,testSize=.2,trainSize=.8, randomState=None)\n",
    "\n",
    "Standardizer = DataHelper.standardizer(trainSet)\n",
    "trainSet = DataHelper.standardizeCompute(trainSet,Standardizer)\n",
    "testSet = DataHelper.standardizeCompute(testSet,Standardizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if specified, the toOneHot will also give us the order of the classfication heads.\n",
    "\n",
    "**Step 4**: Construct the model. Notice that this is the same thing as logistic regression, with the only difference that softmax splits logistic binary outcomes into 2 seperate categories.\n",
    "\n",
    "**Therefore, this is essentially logistic regression, represented with 2 categorical variables instead of a binary variable with a decision threshold:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "                output = FullyConnectedLayer(numNodes=2,activation='softmax')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at how our model does initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Initial Loss: 2.004762491639277\n",
      "Accuracy: 3.65%\n",
      "[1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "loss, guesses = model.evaluate(testSet,testLabels)\n",
    "\n",
    "predictedTrain = np.argmax(guesses, axis=1)\n",
    "trueTrain = np.argmax(testLabels, axis=1)\n",
    "trainAccuracy = np.mean(predictedTrain == trueTrain) * 100\n",
    "\n",
    "print(\"\\n Initial Loss:\", loss)\n",
    "print(\"Accuracy: {:.2f}%\".format(trainAccuracy))\n",
    "print(predictedTrain)\n",
    "print(trueTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that without any sort of training the model can't discern which value is which. Something interesting to note, however, is that the data is so well seperated that the model already has some very high seperation ability. If we were to switch classification heads in this example, we would get a really high accuracy. Let us save this model so we can come back to the original weights, after doing a little bit of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveModel('tutorialModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reload our model as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NeuralNetwork(model='tutorialModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model and report our predictions as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  100 : 0.07723121923789908\n",
      "loss for epoch:  200 : 0.08064279365232271\n",
      "loss for epoch:  300 : 0.08066605935007758\n",
      "loss for epoch:  400 : 0.07786792053886489\n",
      "loss for epoch:  500 : 0.07859150750233064\n",
      "loss for epoch:  600 : 0.08050427394562472\n",
      "loss for epoch:  700 : 0.08089962734148001\n",
      "loss for epoch:  800 : 0.0809128803583\n",
      "loss for epoch:  900 : 0.08144508963522622\n",
      "loss for epoch:  1000 : 0.0787870045531967\n",
      "\n",
      " Testing Loss: 0.07165134527756539\n",
      "Accuracy: 97.08%\n",
      "Predicted values:\n",
      " [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n",
      "True test labels:\n",
      "  [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "model1.train(trainSet, trainLabels) \n",
    "loss1, guesses1 = model1.evaluate(testSet,testLabels)\n",
    "\n",
    "predictedTest = np.argmax(guesses1, axis=1)\n",
    "trueTest = np.argmax(testLabels, axis=1)\n",
    "testAccuracy = np.mean(predictedTest == trueTest) * 100\n",
    "\n",
    "print(\"\\n Testing Loss:\", loss1)\n",
    "print(\"Accuracy: {:.2f}%\".format(testAccuracy))\n",
    "print(\"Predicted values:\\n\" , predictedTest)\n",
    "print(\"True test labels:\\n \", trueTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model now correctly classifies our labels with a fairly good probability. However, notice that the model stayed at roughly the same loss the entire time. This signals that we are doing a lot more training than we need to. We can specify the hyperparameters we want to change to reduce training time with similar values. We can grab from our model of original weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  1 : 0.5605352918596782\n",
      "loss for epoch:  2 : 0.11589345490598545\n",
      "loss for epoch:  3 : 0.09786853191487037\n",
      "loss for epoch:  4 : 0.0906754686300652\n",
      "loss for epoch:  5 : 0.08718543804011794\n",
      "loss for epoch:  6 : 0.08139638979058879\n",
      "loss for epoch:  7 : 0.08068651366497223\n",
      "loss for epoch:  8 : 0.08291385213193285\n",
      "loss for epoch:  9 : 0.08237392201745547\n",
      "loss for epoch:  10 : 0.08103371731942934\n",
      "\n",
      " Model 2's Loss: 0.06565157594302315\n",
      "Accuracy: 97.81%\n",
      "Predicted values:\n",
      " [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n",
      "True test labels:\n",
      "  [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "model2 = NeuralNetwork(model='tutorialModel')\n",
    "model2.train(trainSet, trainLabels , epochs = 10, learningRate=.01) \n",
    "loss2, guesses2 = model2.evaluate(testSet,testLabels)\n",
    "\n",
    "predicted2 = np.argmax(guesses2, axis=1)\n",
    "trueTest = np.argmax(testLabels, axis=1)\n",
    "accuracy2 = np.mean(predicted2 == trueTest) * 100\n",
    "\n",
    "print(\"\\n Model 2's Loss:\", loss2)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"Predicted values:\\n\" , predicted2)\n",
    "print(\"True test labels:\\n \", trueTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get roughly the same testing accuracy with only 20 epochs and even a lower learning rate (.001). But what about Using a neural network instead of simple logistic regression? Well, we can test it out by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLayerModel = NeuralNetwork(\n",
    "                hidden2 = FullyConnectedLayer(numNodes=10,activation='ReLU'),\n",
    "                output = FullyConnectedLayer(numNodes=2,activation='softmax')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a model with 2 hidden layers of 10 nodes each. We specify our activation as ReLU. Let's look at the initial test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Initial Loss: 2.004762491639277\n",
      "Accuracy: 3.65%\n",
      "[1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "lossNN, guessesNN = multiLayerModel.evaluate(testSet,testLabels)\n",
    "\n",
    "predictedNN = np.argmax(guesses, axis=1)\n",
    "trueNN = np.argmax(testLabels, axis=1)\n",
    "accuracyNN = np.mean(predictedNN == trueNN) * 100\n",
    "\n",
    "print(\"\\n Initial Loss:\", loss)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracyNN))\n",
    "print(predictedNN)\n",
    "print(trueNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the extra layer added more variability. Let's see if we can get a better accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  10 : 0.07967712560314037\n",
      "loss for epoch:  20 : 0.06443768278229951\n",
      "loss for epoch:  30 : 0.057603562713047646\n",
      "loss for epoch:  40 : 0.043352506452312554\n",
      "loss for epoch:  50 : 0.03306733031952119\n",
      "loss for epoch:  60 : 0.03382662055128359\n",
      "loss for epoch:  70 : 0.02576288279154249\n",
      "loss for epoch:  80 : 0.02553480721125658\n",
      "loss for epoch:  90 : 0.017964572001106075\n",
      "loss for epoch:  100 : 0.012802245228397867\n",
      "\n",
      " Model 2's Loss: 0.19848948866276955\n",
      "Accuracy: 97.81%\n",
      "Predicted values:\n",
      " [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n",
      "True test labels:\n",
      "  [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "multiLayerModel.train(trainSet, trainLabels , epochs = 100, learningRate=.01) \n",
    "loss2, guesses2 = multiLayerModel.evaluate(testSet,testLabels)\n",
    "\n",
    "predicted2 = np.argmax(guesses2, axis=1)\n",
    "trueTest = np.argmax(testLabels, axis=1)\n",
    "accuracy2 = np.mean(predicted2 == trueTest) * 100\n",
    "\n",
    "print(\"\\n Model 2's Loss:\", loss2)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"Predicted values:\\n\" , predicted2)\n",
    "print(\"True test labels:\\n \", trueTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play around with some regularization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  100 : 0.07118403531023085\n",
      "loss for epoch:  200 : 0.03209212796737963\n",
      "loss for epoch:  300 : 0.07158275560650544\n",
      "loss for epoch:  400 : 0.023710853600024555\n",
      "loss for epoch:  500 : 0.07100867813611793\n",
      "loss for epoch:  600 : 0.06156461314423168\n",
      "loss for epoch:  700 : 0.07743677374999897\n",
      "loss for epoch:  800 : 0.03992193661393943\n",
      "loss for epoch:  900 : 0.039324094557690065\n",
      "loss for epoch:  1000 : 0.04819933432216001\n",
      "\n",
      " Model 2's Loss: 0.24210974031518742\n",
      "Accuracy: 97.08%\n",
      "Predicted values:\n",
      " [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      "True test labels:\n",
      "  [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "multiLayerModel2 = NeuralNetwork(\n",
    "                inputDropout=.01,\n",
    "                hidden2 = FullyConnectedLayer(numNodes=10,activation='ReLU',dropout=.1),\n",
    "                output = FullyConnectedLayer(numNodes=2,activation='softmax')\n",
    "                )\n",
    "multiLayerModel2.train(trainSet, trainLabels , epochs = 1000, learningRate=.01, loss='AdamW', weightDecay=.001) \n",
    "loss2, guesses2 = multiLayerModel2.evaluate(testSet,testLabels)\n",
    "\n",
    "predicted2 = np.argmax(guesses2, axis=1)\n",
    "trueTest = np.argmax(testLabels, axis=1)\n",
    "accuracy2 = np.mean(predicted2 == trueTest) * 100\n",
    "\n",
    "print(\"\\n Model 2's Loss:\", loss2)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"Predicted values:\\n\" , predicted2)\n",
    "print(\"True test labels:\\n \", trueTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a different activation, with more layers of differing nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  1009 : 0.09176907703504801\n",
      "loss for epoch:  2018 : 0.061719116182119074\n",
      "loss for epoch:  3027 : 0.07641179263352049\n",
      "loss for epoch:  4036 : 0.05720140620517416\n",
      "loss for epoch:  5045 : 0.08638890339198194\n",
      "loss for epoch:  6054 : 0.0916573373993677\n",
      "loss for epoch:  7063 : 0.04093082005058776\n",
      "loss for epoch:  8072 : 0.042597118133121996\n",
      "loss for epoch:  9081 : 0.023214693636770867\n",
      "loss for epoch:  10090 : 0.0592108828641166\n",
      "\n",
      " Model 2's Loss: 0.0809297302755145\n",
      "Accuracy: 96.35%\n",
      "Predicted values:\n",
      " [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      "True test labels:\n",
      "  [0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "multiLayerModel3 = NeuralNetwork(\n",
    "                inputDropout=.01,\n",
    "                hidden1 = FullyConnectedLayer(numNodes=10,activation='tanh',dropout=.1),\n",
    "                hidden2 = FullyConnectedLayer(numNodes=10,activation='tanh',dropout=.1),\n",
    "                hidden3 = FullyConnectedLayer(numNodes=7,activation='tanh',dropout=.1),\n",
    "                hidden4 = FullyConnectedLayer(numNodes=5,activation='tanh',dropout=.1),\n",
    "                output = FullyConnectedLayer(numNodes=2,activation='softmax')\n",
    "                )\n",
    "multiLayerModel3.train(trainSet, trainLabels , epochs = 10090, learningRate=.0001, loss='AdamW', weightDecay=.1) \n",
    "loss2, guesses2 = multiLayerModel3.evaluate(testSet,testLabels)\n",
    "\n",
    "predicted2 = np.argmax(guesses2, axis=1)\n",
    "trueTest = np.argmax(testLabels, axis=1)\n",
    "accuracy2 = np.mean(predicted2 == trueTest) * 100\n",
    "\n",
    "print(\"\\n Model 2's Loss:\", loss2)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"Predicted values:\\n\" , predicted2)\n",
    "print(\"True test labels:\\n \", trueTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Let's take one of the features of our dataset, and see if we can predict its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainSet = trainSet[:,:-1]\n",
    "newTrainLabels = trainSet[:,-1:]\n",
    "newTestSet = testSet[:,:-1]\n",
    "newTestLabels = testSet[:,-1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use the entire set as the batch size for full gradient descent. We can specify a regression model as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLayerModel4 = NeuralNetwork(\n",
    "                batchSize=len(newTrainSet),\n",
    "                hidden1 = FullyConnectedLayer(numNodes=10,activation='sigmoid'),\n",
    "                hidden2 = FullyConnectedLayer(numNodes=10,activation='sigmoid'),\n",
    "                output = FullyConnectedLayer(numNodes=1,activation='mse')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our model predicts initially for the first value in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test true Label: [-0.35472957]\n",
      "Guess: [-1.5362973]\n"
     ]
    }
   ],
   "source": [
    "guesses = multiLayerModel4.predict(newTestSet)\n",
    "print(\"Test true Label:\", newTestLabels[0])\n",
    "print(\"Guess:\",guesses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see what our model predicts after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch:  10 : 1.0558547563860399\n",
      "loss for epoch:  20 : 0.7766654785231176\n",
      "loss for epoch:  30 : 0.7469716364359363\n",
      "loss for epoch:  40 : 0.8384351748352712\n",
      "loss for epoch:  50 : 0.658487025089794\n",
      "loss for epoch:  60 : 0.7493767981582926\n",
      "loss for epoch:  70 : 0.8367609143865186\n",
      "loss for epoch:  80 : 0.7444770863316438\n",
      "loss for epoch:  90 : 0.6834451971105168\n",
      "loss for epoch:  100 : 0.6225768136872939\n",
      "Test true Label: [-0.35472957]\n",
      "Guess: [-0.33379226]\n"
     ]
    }
   ],
   "source": [
    "multiLayerModel4.train(newTrainSet, newTrainLabels,epochs=100,learningRate=.01) \n",
    "guesses2 = multiLayerModel4.predict(newTestSet)\n",
    "print(\"Test true Label:\", newTestLabels[0])\n",
    "print(\"Guess:\",guesses2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can use both predict and evaluate for both classification and regression. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss:  2.004762491639277\n",
      "Test true Label: [-0.35472957]\n",
      "Guess: [-0.33379226]\n"
     ]
    }
   ],
   "source": [
    "loss2,guesses2 = multiLayerModel4.evaluate(newTestSet,newTestLabels)\n",
    "print(\"Total Loss: \", loss)\n",
    "print(\"Test true Label:\", newTestLabels[0])\n",
    "print(\"Guess:\",guesses2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much Closer!\n",
    "**Note That these are the standardized values**. If you want to see the actual vs predicted, we can do the following with the Data handler class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual true value:  1.0\n",
      "Guess Value:  1.0364301399213287\n"
     ]
    }
   ],
   "source": [
    "recombinedTest = np.c_[newTestSet,newTestLabels]\n",
    "recombinedGuesses = np.c_[newTestSet,guesses2]\n",
    "\n",
    "actualTest = DataHelper.unNormalize(recombinedTest, Standardizer)\n",
    "actualGuess = DataHelper.unNormalize(recombinedGuesses, Standardizer)\n",
    "\n",
    "print(\"Actual true value: \" , actualTest[0,-1])\n",
    "print(\"Guess Value: \" , actualGuess[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a short tutorial on how to use The Neural Network. When doing classification, do not forget to take the max of a row in order to get the correct prediction. Happy training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features\n",
    "\n",
    "---\n",
    "\n",
    "### Custom Trained Handwritten Digit identifier: \n",
    "\n",
    "Try out drawer.py and see how well the model does that was trained in this neural network architecture with your own drawn numbers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
